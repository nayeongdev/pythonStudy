웹 크롤링은 웹 페이지 내에서 허용된 링크를 계속해서 탐색하고, 
웹 스크래핑은 데이터 추출을 하려는 대상이 명확하여 특정 웹 사이트만 추적한다.
즉, 웹 크롤링은 기존의 복사본을 만들고, 웹 스크래핑은 분석을 위한 특정 데이터를 추출하거나 새로운 것을 만든다.

크롤링 가이드 라인 확인하는법 (안될 수도 있음!)
  사이트 주소뒤에 /robots.txt 붙이고 엔터  ex) youtube.com/robots.txt
  Disallow는 해당 경로에대한 크롤링을 허용하지 않는다는 의미

크롤링은 막 쓰면 안된다.
  계정/IP가 차단될 수 있고, 데이터 무단 활용시 법적 제재가 있을 수 있다.

=============================================================================

XPath (XML Path Language) :XML 문서의 어떤 경로에 접근하기 위해 사용된다. 
XML상에서 특정 부분이 어떤 요소와 속성을 포함하고 있고 그 위치가 어떻게 되는지 파악할 때 주로 사용한다.

F12 -> 가져오고싶은 태그 코드 우클릭 -> copy -> copyXPath
예) 네이버 로그인 버튼 link태그 : //*[@id="account"]/a   
copyfullXPath -> /html/body/div[2]/div[3]/div[3]/div/div[2]/a

=============================================================================

Requests : 주어진 url을 통해 받아온 HTML에 원하는 정보를 가져오기 위해 쓴다
-> [2_requests.py] 파일 참고

res.raise_for_status() 를 통해 접속에 문제 없는지 확인

=============================================================================

정규식 re : 형식을 체크할때 정규식을 쓴다
-> [3_re.py] 파일 참고
1. p = re.compile("원하는 형태")
2. m = p.match("비교할 문자열") : 주어진 문자열의 처음부터 일치하는지 확인
3. m = p.search("비교할 문자열") : 주어진 문자열 중에 일치하는게 있는지 확인
4. lst = p.findall("비교할 문자열") : 일치하는 모든 것을 '리스트' 형태로 반환

원하는 형태 : 정규식으로 표현
. (ca.e) : 하나의 문자를 의미 > care, cafe (o) | caffe (x)
^ (^de)  : 문자열의 시작 > desk, destination (o) | fade (x)
$ (se$)  : 문자열의 끝 > case, base (o) | section(x)

=============================================================================

웹크롤링(스크래핑)시 403에러가 날 수 있는데, 이를 user agent를 통해 해결할 수 있다.
user agent
  웹사이트에 요청하는 소프트웨어/브라우저("에이전트")를 설명하는 텍스트
  웹 브라우저는 웹사이트에 대한 요청에 User Agent 문자열을 포함한다.
  User Agent에는 브라우저/에이전트가 실행되는 운영 체제 및 장치 유형에 대한 설명이 포함된다.

1. google > 'user agent string' 검색
2. what is my user agent? 페이지에서 확인 및 복사
3. 코드 작성
headers = {"User-Agent": "복사한 user agent"}
res = requests.get(url, headers=headers)

If, 한국어를 지원하지만 한국어로 안뜨는 사이트라면
headers = {
  "User-Agent": "복사한 user agent",
  "Accept-Language":"ko-KR,ko"}
=============================================================================

BeautifulSoup4
  HTML과 XML 문서들의 구문을 분석하기 위한 파이썬 패키지다.
  HTML로부터 데이터를 추출하기 위해 사용할 수 있는 파싱된 페이지의 파스 트리를 만드는데, 
  이는 웹 스크래핑에 유용하다.
  [공식 문서] : https://www.crummy.com/software/BeautifulSoup/bs4/doc.ko/

파이썬 패키지 설치
> py -m pip install beautifulsoup4
> py -m pip install lxml

-----------------------------------------------------------------------------
HTTP : HTTP는 클라이언트와 서버 간에 데이터를 주고(요청) 받는(응답) 프로토콜
http method 중 대표적인 2가지 : GET, POST

GET과 POST 차이
- GET은 지정된 리소스에서 데이터를 요청하는 데 사용
  쿼리 문자열(이름/값 쌍)은 GET 요청의 URL로 전송
  길이 제한이 있고 민감한 데이터를 처리할 때 사용해서는 안 된다.
- POST는 리소스를 생성/업데이트하기 위해 서버에 데이터를 보내는 데 사용
  브라우저 기록에 남아 있지 않고 데이터 길이에 대한 제한이 없다.

쿠팡은 GET 방식 이용

-----------------------------------------------------------------------------
find     : 조건에 맞는 첫 번째 element
find_all : 조건에 맞는 모든 element 리스트로
find_next_sibling(s)      : 다음 형제 찾기
find_previous_sibling(s)  : 이전 형제 찾기

soup["속성명"]  : 속성
soup.get_text() : 텍스트

-----------------------------------------------------------------------------
이미지 다운로드
with open("파일명", "wb") as f:
    f.write(res.content)

=============================================================================

csv 파일 만들기 
import csv
f=open(filname,"w",encoding="utf-8-sig",newline="")

=============================================================================

selenium
웹페이지 테스트 자동화 프레임워크
웹 브라우저를 컨트롤하여 자동화를 구현할 수 있는 기능

파이썬 패키지 설치
> py -m pip install selenium

웹드라이브 설치 방법 2가지 (크롬기준)
1. 수동으로 다운로드
크롬 버전 확인 > 주소창에 chrome://version/ 또는 메뉴 > 도움말 > 크롬정보
chromedriver 검색 > https://chromedriver.chromium.org/downloads 접속
버전에 맞는(!중요) 크롬드라이버를 선택하여 운영체제에 맞게 다운로드
다운로드된 압축폴더를 압축해제할 공간(파일)으로 옮겨서 압축풀기
압축해제 후 chromedriver.exe 파일인지 확인

2. 자동으로 다운로드 (아래 코드 추가)
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

service = Service(executable_path=ChromeDriverManager().install())
browser = webdriver.Chrome(service=service, options=options)
또는
browser = webdriver.Chrome(service=Service(
    ChromeDriverManager().install()), options=options)

**1번 방법으로 했지만 실행할때마다 창이 바로 꺼지는 현상이 반복되어서 2번 방법 선택**
+ 창이 안꺼지도록 아래코드도 추가
options = webdriver.ChromeOptions()
options.add_experimental_option("detach", True) # 브라우저 꺼짐 방지
options.add_experimental_option("excludeSwitches", ["enable-logging"])  # 불필요한 에러 메세지 삭제

-----------------------------------------------------------------------------
요소 찾기(https://selenium-python.readthedocs.io/locating-elements.html)
from selenium.webdriver.common.by import By

find_element(By.ID, "id")
find_element(By.NAME, "name")
find_element(By.XPATH, "xpath")
find_element(By.LINK_TEXT, "link text")
find_element(By.PARTIAL_LINK_TEXT, "partial link text")
find_element(By.TAG_NAME, "tag name")
find_element(By.CLASS_NAME, "class name")
find_element(By.CSS_SELECTOR, "css selector")

-----------------------------------------------------------------------------
Enter/Return 키 입력
from selenium.webdriver.common.keys import Keys
sendKeys 메서드를 사용하고 Keys.ENTER(mac은 Keys.RETRUN)를 메서드의 인수로 전달
search.send_keys("예시")
search.send_keys(Keys.ENTER)

-----------------------------------------------------------------------------
현재 탭 닫기 -> browser.close()
전체 브라우저 종료 -> browser.quit()
브라우저 최대창으로 키우기 -> browser.maximize_window()

-----------------------------------------------------------------------------
로딩 있을때 WebDriverWait(browser, 시간초).until(EC.presence_of_element_located((By.형식, "")))
def wait_util(xpath_str):
    WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH, xpath_str)))

-----------------------------------------------------------------------------
headless 크롬
selenium을 이용할때 매번 창을 띄우고 움직임을 확인하였는데, 속도 저하문제 발생 가능성 있음
화면 볼 필요가 없거나 서버에서 웹 스크래핑 작업을 한다면 브라우저를 띄울 필요 없다면 headless 사용
headless 크롬은 크롬을 띄우지 않고 크롬을 쓸 수 있다. (즉, 백그라운드에서 실행)
매번 빠른 속도로 브라우저를 띄우지 않고 똑같은 작업을 할 수 있다.
때로는 user-agent 정의가 필요하다

options = webdriver.ChromeOptions()
options.headless = True
options.add_argument("window-size=1920x1080")

browser=webdriver.Chrome(options=options)